{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A script that generate images in the output folder : non attacked, saliency non attacked, attacked, saliency attacked (with all combinaisons of : models, attacks, effect of attack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_J17H9FPoiVW"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "nYcwSbqeorX6",
        "outputId": "42d3c6f9-51fc-421a-bb4f-d03362b8e296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Load libraries\n",
            "Tensorflow recognized 0 GPUs\n"
          ]
        }
      ],
      "source": [
        "print(\"Executing : Load libraries\")\n",
        "\n",
        "import os\n",
        "# uncomment to force the non use of GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tf_keras_vis.utils import num_of_gpus\n",
        "\n",
        "from keras.utils import load_img, img_to_array, array_to_img\n",
        "\n",
        "import foolbox as fb\n",
        "from foolbox.criteria import TargetedMisclassification\n",
        "from foolbox.attacks import PGD\n",
        "\n",
        "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
        "\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.stats import pearsonr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "_, gpus = num_of_gpus()\n",
        "print('Tensorflow recognized {} GPUs'.format(gpus))\n",
        "\n",
        "path_project_root = os.path.dirname(os.path.abspath(''))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YmybNIO7oztN"
      },
      "source": [
        "# SALIENCY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O1jT7hhgou_o"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name=\"MobileNetV2\"):\n",
        "    \"\"\"\n",
        "        Return the selected model. Argument : name of the model.\n",
        "    \"\"\"\n",
        "    print(\"Executing : Load Model\")\n",
        "    model=\"\"\n",
        "    if model_name==\"VGG16\":\n",
        "        from keras.applications.vgg16 import VGG16 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    elif model_name==\"MobileNetV2\":\n",
        "        from keras.applications.mobilenet_v2 import MobileNetV2 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    elif model_name==\"ResNet152V2\":\n",
        "        from keras.applications.resnet_v2 import ResNet152V2 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    else:\n",
        "        print(\"Error : model not available\")\n",
        "        raise NameError()\n",
        "    return model\n",
        "\n",
        "def get_image(path, size=(224,224), model_name=\"MobileNetV2\"):\n",
        "    print(\"Executing : Load And Preprocess Image\")\n",
        "    image= load_img(path)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = preprocess_image(image, model_name)\n",
        "    return image\n",
        "\n",
        "def preprocess_image(image, model_name):\n",
        "    if model_name==\"VGG16\":\n",
        "        image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    elif model_name==\"MobileNetV2\":\n",
        "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "    elif model_name==\"ResNet152V2\":\n",
        "        image = tf.keras.applications.resnet_v2.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def get_score_function(index_list):\n",
        "    print(\"Executing : Create Score Function\")\n",
        "    return CategoricalScore(index_list)\n",
        "\n",
        "def get_saliency_object(model):\n",
        "    print(\"Executing : Create Saliency Object\")\n",
        "    replace2linear = ReplaceToLinear()\n",
        "    saliency = Saliency(model, model_modifier=replace2linear, clone=True)\n",
        "    return saliency"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZM-CzPo_Ok"
      },
      "source": [
        "# ATTACK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fdkXUR9nowpL"
      },
      "outputs": [],
      "source": [
        "def get_attacked_image_FGSM(image, model, image_index=0, debug = False, attack_rate = 15): #attack_rate entre 0 et 100\n",
        "\n",
        "    pretrained_model = model\n",
        "    pretrained_model.trainable = False\n",
        "\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = image[None, ...]\n",
        "\n",
        "    \n",
        "    if debug:\n",
        "      image_probs = pretrained_model.predict(image)\n",
        "      decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "      _, image_class, class_confidence = decode_predictions(image_probs, top=1)[0][0]\n",
        "      plt.imshow(image[0] * 0.5 + 0.5)  # To change [-1, 1] to [0,1]\n",
        "      plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n",
        "      plt.show()\n",
        "      label = tf.one_hot(image_index, image_probs.shape[-1])\n",
        "      label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(image)\n",
        "      prediction = pretrained_model(image)\n",
        "      loss = loss_object(label, prediction)\n",
        "\n",
        "    # Get the gradients of the loss w.r.t to the input image.\n",
        "    gradient = tape.gradient(loss, image)\n",
        "    # Get the sign of the gradients to create the perturbation\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    perturbations = signed_grad\n",
        "    \n",
        "    #for i, eps in enumerate(epsilons):\n",
        "    adv_x = image + (attack_rate/100)*perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    \n",
        "    if debug :\n",
        "      plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]\n",
        "      plt.title(\"Noise\")\n",
        "      plt.show()\n",
        "       #epsilons = [0, 0.01, 0.1, 0.15] #noter \"taux d'attaque\"\n",
        "      descriptions = ['Epsilon = {:0.3f}'.format(attack_rate)]\n",
        "      _, label, confidence = decode_predictions(pretrained_model.predict(adv_x), top=1)[0][0]\n",
        "      plt.imshow(adv_x[0]*0.5+0.5)\n",
        "      plt.title('{} \\n {} : {:.2f}% Confidence'.format(descriptions[0], label, confidence*100))\n",
        "      plt.show()\n",
        "\n",
        "    attacked_image = adv_x[0]\n",
        "    return attacked_image\n",
        "\n",
        "def get_attacked_image_PGD(model, orig_input, debug = False, attack_rate = 10, class_target = 999) : #class_target c'est la classe que l'on cherche à appliquer\n",
        "    \n",
        "    orig_input = tf.expand_dims(orig_input, 0)\n",
        "    \n",
        "    fmodel = fb.TensorFlowModel(model, bounds=(-255, 255))\n",
        "\n",
        "   \n",
        "    #goldfish = 1; brown bear = 294; assault rifle = 413 #class de notre image\n",
        "    # 2 = great white shark #classe que l'on cherche à obtenir\n",
        "    adv_label = tf.convert_to_tensor([class_target])\n",
        "\n",
        "    criterion = TargetedMisclassification(adv_label)\n",
        "\n",
        "\n",
        "    attack = PGD()\n",
        "    input_as_tensor = tf.convert_to_tensor(orig_input)\n",
        "    adv_input = attack.run(fmodel, input_as_tensor, criterion, epsilon=attack_rate)\n",
        "\n",
        "    adv_img = (adv_input.numpy() + 255) / 2\n",
        "    adv_img = adv_img.reshape(224, 224, 3)\n",
        "    adv_img = array_to_img(adv_img)\n",
        "    b, g, r = adv_img.split()\n",
        "    adv_img = Image.merge(\"RGB\", (r, g, b))\n",
        "\n",
        "    \n",
        "    \n",
        "    if debug :\n",
        "      adv_input = img_to_array(adv_img)\n",
        "      adv_input = adv_input.reshape((1, 224, 224, 3))\n",
        "      adv_input = preprocess_input(adv_input)\n",
        "      predictions = model.predict(adv_input)\n",
        "      labels = decode_predictions(predictions)\n",
        "      kind = labels[0][0][1].replace(\"_\", \" \").title()\n",
        "      percent = round(labels[0][0][2] * 100, 2)\n",
        "      print(f\"This is a {kind}. I am {percent} % sure.\")\n",
        "      print()\n",
        "      print(\"Other suggestions:\")\n",
        "      for i in range(4):\n",
        "        kind = labels[0][i+1][1].replace(\"_\", \" \").title()\n",
        "        percent = round(labels[0][i+1][2] * 100, 2)\n",
        "        print(f\"{kind}: {percent} %\")\n",
        "      difference = ImageChops.difference(adv_img, orig_input)\n",
        "      plt.figure()\n",
        "      plt.imshow(np.array(difference))\n",
        "      plt.show()\n",
        "      difference = ImageEnhance.Brightness(difference).enhance(10)\n",
        "      plt.figure()\n",
        "      plt.imshow(np.array(difference))\n",
        "      plt.show()\n",
        "\n",
        "    return adv_img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfLYblzpAnG"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EfurSXi7oyXw"
      },
      "outputs": [],
      "source": [
        "# Ouvrir les cartes de saillance\n",
        "def img_open(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    return img1,img2\n",
        "\n",
        "# Calcul de la différence absolue\n",
        "# Plus la valeur est proche de 0 plus les images sont similaires\n",
        "def diff_abs(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    return np.sum(np.abs(img1 - img2))\n",
        "\n",
        "# Calcul de la différence quadratique (mse)\n",
        "# Plus la valeur est proche de 0 plus les images sont similaires\n",
        "def diff_quadratique(path_sal_map,path_attacked_sal_map):\n",
        "    img1, img2 = img_open(path_sal_map,path_attacked_sal_map)\n",
        "    return np.sum(np.square(img1 - img2))\n",
        "\n",
        "# Calcul du coef de corrélation\n",
        "# Plus la valeur est proche de 1 plus les images sont similaires\n",
        "def coef_correlation(path_sal_map,path_attacked_sal_map):\n",
        "    img1, img2 = img_open(path_sal_map,path_attacked_sal_map)\n",
        "    result = pearsonr(img1.flatten(),img2.flatten())\n",
        "    return result.statistic\n",
        "\n",
        "# Calcul du ssim Structural Similarity Index\n",
        "# Plus la valeur est proche de 1 plus les images sont similaires\n",
        "def ssim_func(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    gray_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "    valssim = ssim(gray_img1, gray_img2)\n",
        "    return valssim\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FrLezdjTpESi"
      },
      "source": [
        "# MAIN PROGRAM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If 1 image is 100Ko => 40Mo generated for each input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n"
          ]
        }
      ],
      "source": [
        "models_availables = [ \"VGG16\", \"ResNet152V2\", \"MobileNetV2\"]\n",
        "attacks_availables = [ \"PGD\", \"FGSM\"]\n",
        "input_directory  = os.path.join(path_project_root, 'final_dir\\images\\input')\n",
        "output_directory = os.path.join(path_project_root, 'final_dir\\images\\output\\\\')\n",
        "index_images_model = [404, 294, 500, 1, 576, 587, 606, 413]\n",
        "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
        "\n",
        "score_list = list()\n",
        "for value in index_images_model:\n",
        "    score_list.append(get_score_function([value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Load Model\n",
            "Executing : Create Saliency Object\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 320ms/step\n",
            "1/1 [==============================] - 0s 315ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 168ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 204ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m attacked_image\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m attack_name\u001b[39m==\u001b[39mattacks_availables[\u001b[39m0\u001b[39m]:\n\u001b[1;32m---> 19\u001b[0m     attacked_image \u001b[39m=\u001b[39m get_attacked_image_PGD(model, orig_input\u001b[39m=\u001b[39;49moriginal_image, attack_rate\u001b[39m=\u001b[39;49mattack_rate)\n\u001b[0;32m     20\u001b[0m \u001b[39melif\u001b[39;00m attack_name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFGSM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     21\u001b[0m     attacked_image \u001b[39m=\u001b[39m get_attacked_image_FGSM(image\u001b[39m=\u001b[39moriginal_image, model\u001b[39m=\u001b[39mmodel, attack_rate\u001b[39m=\u001b[39mattack_rate)\n",
            "Cell \u001b[1;32mIn[5], line 68\u001b[0m, in \u001b[0;36mget_attacked_image_PGD\u001b[1;34m(model, orig_input, debug, attack_rate, class_target)\u001b[0m\n\u001b[0;32m     66\u001b[0m attack \u001b[39m=\u001b[39m PGD()\n\u001b[0;32m     67\u001b[0m input_as_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(orig_input)\n\u001b[1;32m---> 68\u001b[0m adv_input \u001b[39m=\u001b[39m attack\u001b[39m.\u001b[39;49mrun(fmodel, input_as_tensor, criterion, epsilon\u001b[39m=\u001b[39;49mattack_rate)\n\u001b[0;32m     70\u001b[0m adv_img \u001b[39m=\u001b[39m (adv_input\u001b[39m.\u001b[39mnumpy() \u001b[39m+\u001b[39m \u001b[39m255\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     71\u001b[0m adv_img \u001b[39m=\u001b[39m adv_img\u001b[39m.\u001b[39mreshape(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py:155\u001b[0m, in \u001b[0;36mBaseGradientDescent.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    152\u001b[0m     x \u001b[39m=\u001b[39m x0\n\u001b[0;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m--> 155\u001b[0m     _, gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_and_grad(loss_fn, x)\n\u001b[0;32m    156\u001b[0m     gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(gradients, x\u001b[39m=\u001b[39mx, bounds\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mbounds)\n\u001b[0;32m    157\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m gradient_step_sign \u001b[39m*\u001b[39m optimizer(gradients)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py:111\u001b[0m, in \u001b[0;36mBaseGradientDescent.value_and_grad\u001b[1;34m(self, loss_fn, x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[39m# can be overridden by users\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    108\u001b[0m     loss_fn: Callable[[ep\u001b[39m.\u001b[39mTensor], ep\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m    109\u001b[0m     x: ep\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    110\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[ep\u001b[39m.\u001b[39mTensor, ep\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m ep\u001b[39m.\u001b[39;49mvalue_and_grad(loss_fn, x)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\framework.py:360\u001b[0m, in \u001b[0;36mvalue_and_grad\u001b[1;34m(f, t, *args, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    358\u001b[0m     f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, TensorType], t: TensorType, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    359\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 360\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mvalue_and_grad(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\tensor\\tensor.py:553\u001b[0m, in \u001b[0;36mTensor.value_and_grad\u001b[1;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    551\u001b[0m     \u001b[39mself\u001b[39m: TensorType, f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, TensorType], \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    552\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_and_grad_fn(f, has_aux\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\tensor\\tensorflow.py:481\u001b[0m, in \u001b[0;36mTensorFlowTensor._value_and_grad_fn.<locals>.value_and_grad\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    479\u001b[0m         loss, aux \u001b[39m=\u001b[39m f(x_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    480\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m         loss \u001b[39m=\u001b[39m f(x_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    482\u001b[0m grad \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss\u001b[39m.\u001b[39mraw, x_\u001b[39m.\u001b[39mraw)\n\u001b[0;32m    483\u001b[0m grad \u001b[39m=\u001b[39m TensorFlowTensor(grad)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py:96\u001b[0m, in \u001b[0;36mBaseGradientDescent.get_loss_fn.<locals>.loss_fn\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(inputs: ep\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ep\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 96\u001b[0m     logits \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m ep\u001b[39m.\u001b[39mcrossentropy(logits, labels)\u001b[39m.\u001b[39msum()\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\models\\base.py:102\u001b[0m, in \u001b[0;36mModelWithPreprocessing.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    100\u001b[0m x, restore_type \u001b[39m=\u001b[39m ep\u001b[39m.\u001b[39mastensor_(inputs)\n\u001b[0;32m    101\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess(x)\n\u001b[1;32m--> 102\u001b[0m z \u001b[39m=\u001b[39m ep\u001b[39m.\u001b[39mastensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(y\u001b[39m.\u001b[39;49mraw))\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m restore_type(z)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\engine\\training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\engine\\base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1144\u001b[0m ):\n\u001b[1;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\engine\\functional.py:512\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\engine\\functional.py:669\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 669\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    671\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    673\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    674\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\engine\\base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1144\u001b[0m ):\n\u001b[1;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\keras\\layers\\core\\dense.py:241\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m         outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39membedding_lookup_sparse(\n\u001b[0;32m    238\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, ids, weights, combiner\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m         outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(a\u001b[39m=\u001b[39;49minputs, b\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel)\n\u001b[0;32m    242\u001b[0m \u001b[39m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtensordot(inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, [[rank \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m]])\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3766\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3763\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[0;32m   3764\u001b[0m       a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   3765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3766\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(\n\u001b[0;32m   3767\u001b[0m       a, b, transpose_a\u001b[39m=\u001b[39;49mtranspose_a, transpose_b\u001b[39m=\u001b[39;49mtranspose_b, name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6713\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6711\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6712\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6713\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6714\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   6715\u001b[0m       transpose_b)\n\u001b[0;32m   6716\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6717\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for model_name in models_availables:\n",
        "    model = load_model(model_name)\n",
        "    saliency = get_saliency_object(model)\n",
        "    for attack_name in attacks_availables:\n",
        "        for attack_rate in epsilons:#range(0, 100, 2):\n",
        "            image_index = 0\n",
        "            for filename in os.listdir(input_directory):\n",
        "                filepath = os.path.join(input_directory, filename)\n",
        "                if os.path.isfile(filepath):\n",
        "\n",
        "                    original_image = get_image(filepath, model_name)\n",
        "                    print(\"Executing : Saliency\")\n",
        "                    saliency_image = saliency(score_list[image_index], original_image)[0]\n",
        "\n",
        "\n",
        "                    print(\"Executing : Attack \" + attack_name)\n",
        "                    attacked_image=\"\"\n",
        "                    if attack_name==attacks_availables[0]:\n",
        "                        attacked_image = get_attacked_image_PGD(model, orig_input=original_image, attack_rate=attack_rate)\n",
        "                    elif attack_name==\"FGSM\":\n",
        "                        attacked_image = get_attacked_image_FGSM(image=original_image, model=model, attack_rate=attack_rate)\n",
        "                    \n",
        "                    # plt.imshow(np.array(attacked_image))\n",
        "                    # plt.show()\n",
        "                    # print(np.shape(attacked_image))\n",
        "                    print(\"Executing : Saliency on Attacked image\")\n",
        "                    attacked_image_array_preprocessed = preprocess_image(np.array(attacked_image).astype(np.float32), model_name)\n",
        "                    attacked_saliency_image = saliency(score_list[image_index], attacked_image_array_preprocessed)[0]\n",
        "\n",
        "                    print(\"Executing : Prediction attacked image\")\n",
        "                    img_attacked_to_predict = np.resize(attacked_image_array_preprocessed, (1, 224, 224, 3))\n",
        "                    predictions_attacked = model.predict(img_attacked_to_predict)\n",
        "                    prediction_attacked = round(predictions_attacked[0][index_images_model[image_index]]*1000000)\n",
        "\n",
        "                    original_image = cv2.imread(filepath)\n",
        "                    img_original_to_predict = np.resize(original_image, (1, 224, 224, 3))\n",
        "                    predictions_original = model.predict(img_original_to_predict)\n",
        "                    prediction_original = round(predictions_original[0][index_images_model[image_index]]*1000000)\n",
        "\n",
        "                    filename_output = filename[0:-4] + \"_\" + model_name + \"_\" + attack_name + \"_\" + str(attack_rate)\n",
        "\n",
        "\n",
        "                    print(\"Executing : Save images\")\n",
        "                    save_original_image = np.array(original_image)\n",
        "                    save_attacked_image = np.array(attacked_image)\n",
        "                    save_saliency_image = 255*np.array(saliency_image)\n",
        "                    save_attacked_saliency_image = 255*np.array(attacked_saliency_image)\n",
        "                    cv2.imwrite(output_directory + filename_output + \"_\" + str(prediction_original) + \"_nosaliency_noattacked\" + \".png\", save_original_image)\n",
        "                    cv2.imwrite(output_directory + filename_output + \"_\" + str(prediction_attacked) + \"_nosaliency_attacked\"   + \".png\", save_attacked_image)\n",
        "                    cv2.imwrite(output_directory + filename_output + \"_\" + \"-1\"                     + \"_saliency_noattacked\"   + \".png\", save_saliency_image)\n",
        "                    cv2.imwrite(output_directory + filename_output + \"_\" + \"-1\"                     + \"_saliency_attacked\"     + \".png\", save_attacked_saliency_image)\n",
        "                    \n",
        "                    image_index+=1\n",
        "                    print(\"-----------------------------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
