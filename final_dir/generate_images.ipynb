{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A script that generate images in the output folder : non attacked, saliency non attacked, attacked, saliency attacked (with all combinaisons of : models, attacks, effect of attack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_J17H9FPoiVW"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "nYcwSbqeorX6",
        "outputId": "42d3c6f9-51fc-421a-bb4f-d03362b8e296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Load libraries\n",
            "Tensorflow recognized 0 GPUs\n"
          ]
        }
      ],
      "source": [
        "print(\"Executing : Load libraries\")\n",
        "\n",
        "import os\n",
        "# uncomment to force the non use of GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tf_keras_vis.utils import num_of_gpus\n",
        "\n",
        "from keras.utils import load_img, img_to_array, array_to_img\n",
        "\n",
        "import foolbox as fb\n",
        "from foolbox.criteria import TargetedMisclassification\n",
        "from foolbox.attacks import PGD\n",
        "\n",
        "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
        "\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.stats import pearsonr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "_, gpus = num_of_gpus()\n",
        "print('Tensorflow recognized {} GPUs'.format(gpus))\n",
        "\n",
        "path_project_root = os.path.dirname(os.path.abspath(''))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YmybNIO7oztN"
      },
      "source": [
        "# SALIENCY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "O1jT7hhgou_o"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name=\"MobileNetV2\"):\n",
        "    \"\"\"\n",
        "        Return the selected model. Argument : name of the model.\n",
        "    \"\"\"\n",
        "    print(\"Executing : Load Model\")\n",
        "    model=\"\"\n",
        "    if model_name==\"VGG16\":\n",
        "        from keras.applications.vgg16 import VGG16 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    elif model_name==\"MobileNetV2\":\n",
        "        from keras.applications.mobilenet_v2 import MobileNetV2 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    elif model_name==\"ResNet152V2\":\n",
        "        from keras.applications.resnet_v2 import ResNet152V2 as Model\n",
        "        model = Model(weights='imagenet', include_top=True)\n",
        "    else:\n",
        "        print(\"Error : model not available\")\n",
        "        raise NameError()\n",
        "    return model\n",
        "\n",
        "def get_image(path, size=(224,224), model_name=\"MobileNetV2\"):\n",
        "    print(\"Executing : Load And Preprocess Image\")\n",
        "    image= load_img(path)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = preprocess_image(image, model_name)\n",
        "    return image\n",
        "\n",
        "def preprocess_image(image, model_name):\n",
        "    if model_name==\"VGG16\":\n",
        "        image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "    elif model_name==\"MobileNetV2\":\n",
        "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "    elif model_name==\"ResNet152V2\":\n",
        "        image = tf.keras.applications.resnet_v2.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def get_score_function(index_list):\n",
        "    print(\"Executing : Create Score Function\")\n",
        "    return CategoricalScore(index_list)\n",
        "\n",
        "def get_saliency_object(model):\n",
        "    print(\"Executing : Create Saliency Object\")\n",
        "    replace2linear = ReplaceToLinear()\n",
        "    saliency = Saliency(model, model_modifier=replace2linear, clone=True)\n",
        "    return saliency"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZM-CzPo_Ok"
      },
      "source": [
        "# ATTACK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "fdkXUR9nowpL"
      },
      "outputs": [],
      "source": [
        "def get_attacked_image_FGSM(image, model, image_index=0, debug = False, attack_rate = 15): #attack_rate entre 0 et 100\n",
        "\n",
        "    pretrained_model = model\n",
        "    pretrained_model.trainable = False\n",
        "\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = image[None, ...]\n",
        "\n",
        "    \n",
        "    if debug:\n",
        "      image_probs = pretrained_model.predict(image)\n",
        "      decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
        "      _, image_class, class_confidence = decode_predictions(image_probs, top=1)[0][0]\n",
        "      plt.imshow(image[0] * 0.5 + 0.5)  # To change [-1, 1] to [0,1]\n",
        "      plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n",
        "      plt.show()\n",
        "      label = tf.one_hot(image_index, image_probs.shape[-1])\n",
        "      label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(image)\n",
        "      prediction = pretrained_model(image)\n",
        "      loss = loss_object(label, prediction)\n",
        "\n",
        "    # Get the gradients of the loss w.r.t to the input image.\n",
        "    gradient = tape.gradient(loss, image)\n",
        "    # Get the sign of the gradients to create the perturbation\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    perturbations = signed_grad\n",
        "    \n",
        "    #for i, eps in enumerate(epsilons):\n",
        "    adv_x = image + (attack_rate/100)*perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    \n",
        "    if debug :\n",
        "      plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]\n",
        "      plt.title(\"Noise\")\n",
        "      plt.show()\n",
        "       #epsilons = [0, 0.01, 0.1, 0.15] #noter \"taux d'attaque\"\n",
        "      descriptions = ['Epsilon = {:0.3f}'.format(attack_rate)]\n",
        "      _, label, confidence = decode_predictions(pretrained_model.predict(adv_x), top=1)[0][0]\n",
        "      plt.imshow(adv_x[0]*0.5+0.5)\n",
        "      plt.title('{} \\n {} : {:.2f}% Confidence'.format(descriptions[0], label, confidence*100))\n",
        "      plt.show()\n",
        "\n",
        "    attacked_image = adv_x[0]\n",
        "    return attacked_image\n",
        "\n",
        "def get_attacked_image_PGD(model, orig_input, debug = False, attack_rate = 10, class_target = 999) : #class_target c'est la classe que l'on cherche à appliquer\n",
        "    \n",
        "    orig_input = tf.expand_dims(orig_input, 0)\n",
        "    \n",
        "    fmodel = fb.TensorFlowModel(model, bounds=(-255, 255))\n",
        "\n",
        "   \n",
        "    #goldfish = 1; brown bear = 294; assault rifle = 413 #class de notre image\n",
        "    # 2 = great white shark #classe que l'on cherche à obtenir\n",
        "    adv_label = tf.convert_to_tensor([class_target])\n",
        "\n",
        "    criterion = TargetedMisclassification(adv_label)\n",
        "\n",
        "\n",
        "    attack = PGD()\n",
        "    input_as_tensor = tf.convert_to_tensor(orig_input)\n",
        "    adv_input = attack.run(fmodel, input_as_tensor, criterion, epsilon=attack_rate)\n",
        "\n",
        "    adv_img = (adv_input.numpy() + 255) / 2\n",
        "    adv_img = adv_img.reshape(224, 224, 3)\n",
        "    adv_img = array_to_img(adv_img)\n",
        "    b, g, r = adv_img.split()\n",
        "    adv_img = Image.merge(\"RGB\", (r, g, b))\n",
        "\n",
        "    \n",
        "    \n",
        "    if debug :\n",
        "      adv_input = img_to_array(adv_img)\n",
        "      adv_input = adv_input.reshape((1, 224, 224, 3))\n",
        "      adv_input = preprocess_input(adv_input)\n",
        "      predictions = model.predict(adv_input)\n",
        "      labels = decode_predictions(predictions)\n",
        "      kind = labels[0][0][1].replace(\"_\", \" \").title()\n",
        "      percent = round(labels[0][0][2] * 100, 2)\n",
        "      print(f\"This is a {kind}. I am {percent} % sure.\")\n",
        "      print()\n",
        "      print(\"Other suggestions:\")\n",
        "      for i in range(4):\n",
        "        kind = labels[0][i+1][1].replace(\"_\", \" \").title()\n",
        "        percent = round(labels[0][i+1][2] * 100, 2)\n",
        "        print(f\"{kind}: {percent} %\")\n",
        "      difference = ImageChops.difference(adv_img, orig_input)\n",
        "      plt.figure()\n",
        "      plt.imshow(np.array(difference))\n",
        "      plt.show()\n",
        "      difference = ImageEnhance.Brightness(difference).enhance(10)\n",
        "      plt.figure()\n",
        "      plt.imshow(np.array(difference))\n",
        "      plt.show()\n",
        "\n",
        "    return adv_img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfLYblzpAnG"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EfurSXi7oyXw"
      },
      "outputs": [],
      "source": [
        "# Ouvrir les cartes de saillance\n",
        "def img_open(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    return img1,img2\n",
        "\n",
        "# Calcul de la différence absolue\n",
        "# Plus la valeur est proche de 0 plus les images sont similaires\n",
        "def diff_abs(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    return np.sum(np.abs(img1 - img2))\n",
        "\n",
        "# Calcul de la différence quadratique (mse)\n",
        "# Plus la valeur est proche de 0 plus les images sont similaires\n",
        "def diff_quadratique(path_sal_map,path_attacked_sal_map):\n",
        "    img1, img2 = img_open(path_sal_map,path_attacked_sal_map)\n",
        "    return np.sum(np.square(img1 - img2))\n",
        "\n",
        "# Calcul du coef de corrélation\n",
        "# Plus la valeur est proche de 1 plus les images sont similaires\n",
        "def coef_correlation(path_sal_map,path_attacked_sal_map):\n",
        "    img1, img2 = img_open(path_sal_map,path_attacked_sal_map)\n",
        "    result = pearsonr(img1.flatten(),img2.flatten())\n",
        "    return result.statistic\n",
        "\n",
        "# Calcul du ssim Structural Similarity Index\n",
        "# Plus la valeur est proche de 1 plus les images sont similaires\n",
        "def ssim_func(path_sal_map,path_attacked_sal_map):\n",
        "    img1 = cv2.imread(path_sal_map)\n",
        "    img2 = cv2.imread(path_attacked_sal_map)\n",
        "    gray_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "    valssim = ssim(gray_img1, gray_img2)\n",
        "    return valssim\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FrLezdjTpESi"
      },
      "source": [
        "# MAIN PROGRAM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If 1 image is 100Ko => 40Mo generated for each input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n",
            "Executing : Create Score Function\n"
          ]
        }
      ],
      "source": [
        "models_availables = [ \"VGG16\", \"ResNet152V2\", \"MobileNetV2\"]\n",
        "attacks_availables = [ \"PGD\", \"FGSM\"]\n",
        "input_directory  = os.path.join(path_project_root, 'final_dir\\images\\input')\n",
        "output_directory = os.path.join(path_project_root, 'final_dir\\images\\output\\\\')\n",
        "index_images_model = [404, 294, 500, 1, 576, 587, 606, 413]\n",
        "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
        "\n",
        "score_list = list()\n",
        "for value in index_images_model:\n",
        "    score_list.append(get_score_function([value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing : Load Model\n",
            "Executing : Create Saliency Object\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 316ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n",
            "Executing : Saliency on Attacked image\n",
            "Executing : Prediction attacked image\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "Executing : Save images\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "Executing : Load And Preprocess Image\n",
            "Executing : Saliency\n",
            "Executing : Attack PGD\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m attacked_image\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m attack_name\u001b[39m==\u001b[39mattacks_availables[\u001b[39m0\u001b[39m]:\n\u001b[1;32m---> 19\u001b[0m     attacked_image \u001b[39m=\u001b[39m get_attacked_image_PGD(model, orig_input\u001b[39m=\u001b[39;49moriginal_image, attack_rate\u001b[39m=\u001b[39;49mattack_rate)\n\u001b[0;32m     20\u001b[0m \u001b[39melif\u001b[39;00m attack_name\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFGSM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     21\u001b[0m     attacked_image \u001b[39m=\u001b[39m get_attacked_image_FGSM(image\u001b[39m=\u001b[39moriginal_image, model\u001b[39m=\u001b[39mmodel, attack_rate\u001b[39m=\u001b[39mattack_rate)\n",
            "Cell \u001b[1;32mIn[49], line 68\u001b[0m, in \u001b[0;36mget_attacked_image_PGD\u001b[1;34m(model, orig_input, debug, attack_rate, class_target)\u001b[0m\n\u001b[0;32m     66\u001b[0m attack \u001b[39m=\u001b[39m PGD()\n\u001b[0;32m     67\u001b[0m input_as_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(orig_input)\n\u001b[1;32m---> 68\u001b[0m adv_input \u001b[39m=\u001b[39m attack\u001b[39m.\u001b[39;49mrun(fmodel, input_as_tensor, criterion, epsilon\u001b[39m=\u001b[39;49mattack_rate)\n\u001b[0;32m     70\u001b[0m adv_img \u001b[39m=\u001b[39m (adv_input\u001b[39m.\u001b[39mnumpy() \u001b[39m+\u001b[39m \u001b[39m255\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     71\u001b[0m adv_img \u001b[39m=\u001b[39m adv_img\u001b[39m.\u001b[39mreshape(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py:155\u001b[0m, in \u001b[0;36mBaseGradientDescent.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    152\u001b[0m     x \u001b[39m=\u001b[39m x0\n\u001b[0;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m--> 155\u001b[0m     _, gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_and_grad(loss_fn, x)\n\u001b[0;32m    156\u001b[0m     gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(gradients, x\u001b[39m=\u001b[39mx, bounds\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mbounds)\n\u001b[0;32m    157\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m gradient_step_sign \u001b[39m*\u001b[39m optimizer(gradients)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py:111\u001b[0m, in \u001b[0;36mBaseGradientDescent.value_and_grad\u001b[1;34m(self, loss_fn, x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[39m# can be overridden by users\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    108\u001b[0m     loss_fn: Callable[[ep\u001b[39m.\u001b[39mTensor], ep\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m    109\u001b[0m     x: ep\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    110\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[ep\u001b[39m.\u001b[39mTensor, ep\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m ep\u001b[39m.\u001b[39;49mvalue_and_grad(loss_fn, x)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\framework.py:360\u001b[0m, in \u001b[0;36mvalue_and_grad\u001b[1;34m(f, t, *args, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    358\u001b[0m     f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, TensorType], t: TensorType, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    359\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 360\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mvalue_and_grad(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\tensor\\tensor.py:553\u001b[0m, in \u001b[0;36mTensor.value_and_grad\u001b[1;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_and_grad\u001b[39m(\n\u001b[0;32m    551\u001b[0m     \u001b[39mself\u001b[39m: TensorType, f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, TensorType], \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    552\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_and_grad_fn(f, has_aux\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\eagerpy\\tensor\\tensorflow.py:482\u001b[0m, in \u001b[0;36mTensorFlowTensor._value_and_grad_fn.<locals>.value_and_grad\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m         loss \u001b[39m=\u001b[39m f(x_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 482\u001b[0m grad \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss\u001b[39m.\u001b[39;49mraw, x_\u001b[39m.\u001b[39;49mraw)\n\u001b[0;32m    483\u001b[0m grad \u001b[39m=\u001b[39m TensorFlowTensor(grad)\n\u001b[0;32m    484\u001b[0m \u001b[39massert\u001b[39;00m grad\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m x_\u001b[39m.\u001b[39mshape\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1059\u001b[0m           output_gradients))\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1065\u001b[0m     flat_targets,\n\u001b[0;32m   1066\u001b[0m     flat_sources,\n\u001b[0;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:146\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:592\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    573\u001b[0m shape_0, shape_1 \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_n([op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m], op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]])\n\u001b[0;32m    575\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[39m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[39m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[39m# in Eager mode.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    582\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d_backprop_input(\n\u001b[0;32m    583\u001b[0m         shape_0,\n\u001b[0;32m    584\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m],\n\u001b[0;32m    585\u001b[0m         grad,\n\u001b[0;32m    586\u001b[0m         dilations\u001b[39m=\u001b[39mdilations,\n\u001b[0;32m    587\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[0;32m    588\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m    589\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mexplicit_paddings,\n\u001b[0;32m    590\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39muse_cudnn_on_gpu,\n\u001b[0;32m    591\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format),\n\u001b[1;32m--> 592\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_filter(\n\u001b[0;32m    593\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    594\u001b[0m         shape_1,\n\u001b[0;32m    595\u001b[0m         grad,\n\u001b[0;32m    596\u001b[0m         dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[0;32m    597\u001b[0m         strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[0;32m    598\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m    599\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mexplicit_paddings,\n\u001b[0;32m    600\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49muse_cudnn_on_gpu,\n\u001b[0;32m    601\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format)\n\u001b[0;32m    602\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\Arthur\\anaconda3\\envs\\vis-keras\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:1369\u001b[0m, in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1368\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1369\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1370\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2DBackpropFilter\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, filter_sizes, out_backprop,\n\u001b[0;32m   1371\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides, \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1372\u001b[0m       padding, \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1373\u001b[0m       data_format, \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[0;32m   1374\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1375\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for model_name in models_availables:\n",
        "    model = load_model(model_name)\n",
        "    saliency = get_saliency_object(model)\n",
        "    for attack_name in attacks_availables:\n",
        "        for attack_rate in epsilons:#range(0, 100, 2):\n",
        "            image_index = 0\n",
        "            for filename in os.listdir(input_directory):\n",
        "                filepath = os.path.join(input_directory, filename)\n",
        "                if os.path.isfile(filepath):\n",
        "\n",
        "                    original_image = get_image(filepath, model_name)\n",
        "                    print(\"Executing : Saliency\")\n",
        "                    saliency_image = saliency(score_list[image_index], original_image)[0]\n",
        "\n",
        "\n",
        "                    print(\"Executing : Attack \" + attack_name)\n",
        "                    attacked_image=\"\"\n",
        "                    if attack_name==attacks_availables[0]:\n",
        "                        attacked_image = get_attacked_image_PGD(model, orig_input=original_image, attack_rate=attack_rate)\n",
        "                    elif attack_name==\"FGSM\":\n",
        "                        attacked_image = get_attacked_image_FGSM(image=original_image, model=model, attack_rate=attack_rate)\n",
        "                    \n",
        "                    # plt.imshow(np.array(attacked_image))\n",
        "                    # plt.show()\n",
        "                    # print(np.shape(attacked_image))\n",
        "                    print(\"Executing : Saliency on Attacked image\")\n",
        "                    attacked_image_array_preprocessed = preprocess_image(np.array(attacked_image).astype(np.float32), model_name)\n",
        "                    attacked_saliency_image = saliency(score_list[image_index], attacked_image_array_preprocessed)[0]\n",
        "\n",
        "                    print(\"Executing : Prediction attacked image\")\n",
        "                    img_attacked_to_predict = np.resize(attacked_image_array_preprocessed, (1, 224, 224, 3))\n",
        "                    predictions = model.predict(img_attacked_to_predict)\n",
        "                    prediction = round(predictions[0][index_images_model[image_index]]*1000000)\n",
        "\n",
        "                    filename_output = filename[0:-4] + \"_\" + model_name + \"_\" + attack_name + \"_\" + str(attack_rate)\n",
        "\n",
        "\n",
        "                    print(\"Executing : Save images\")\n",
        "                    attacked_image = cv2.cvtColor(np.array(attacked_image), cv2.COLOR_BGR2RGB)\n",
        "                    plt.imsave(output_directory + filename_output + \"_\" + str(prediction) + \"_nosaliency_attacked\"   + \".png\", np.array(attacked_image))\n",
        "                    plt.imsave(output_directory + filename_output + \"-1\"                  + \"_saliency_noattacked\"   + \".png\", saliency_image)\n",
        "                    plt.imsave(output_directory + filename_output + \"-1\"                  + \"_saliency_attacked\"     + \".png\", attacked_saliency_image)\n",
        "                    \n",
        "                    image_index+=1\n",
        "                    print(\"-----------------------------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
